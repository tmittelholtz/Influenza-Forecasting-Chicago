{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "857c34d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "conn = sqlite3.connect(\"influenza.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d7418a",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "The data we'll use is publicly available online, and covers the years 2010-2019 in the Chicago area (Cook County, IL). We will import the following CVSs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f668ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"weather4.csv\").to_sql(\"weather\", conn, index=False, if_exists=\"replace\")\n",
    "pd.read_csv(\"google_trends.csv\").to_sql(\"google_trends\", conn, index=False, if_exists=\"replace\")\n",
    "pd.read_csv(\"vaccination_rates2.csv\").to_sql(\"vaccination_rates\", conn, index=False, if_exists=\"replace\")\n",
    "pd.read_csv(\"positive_tests.csv\").to_sql(\"positive_tests\", conn, index=False, if_exists=\"replace\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9a83a0",
   "metadata": {},
   "source": [
    "**Weather:** From meteostat.net, specifically the Chicago O'Hare International Airport (ORD) station. It contains over 10 different daily metrics. We'll look at precipitation and temperature only, since those are widely considered the most relevant to flu spread. These are our most granular features, so we will aggregate weekly information to match our target variable intervals.\n",
    "\n",
    "**Google Trends:** This is a publicly available tool that Google provides. The imported CSV contains search volume by month for different flu-related searches in the Chicago area. We will need to forward-fill data to match our weekly target variable. We need to account for the increase in usage over the 9-year time period, but specific U.S. search volume history is not released by Google. However, it can be found by aggregating monthly volume reports on comscore.com.\n",
    "\n",
    "**Vaccination Rates:** The CDC releases influenza vaccination data each year. While data isn't available for the Chicago area specifically, we can use Illinois to infer vaccination rates. This data is also monthly, represented by percent of the population currently vaccinated, and so will need to be forward-filled. The vaccination rates given reset in August, so we will have to line it up with our selected season cutoff of 10/1. The dataset itself is large and counts certain groups multiple times. To get the most general rate, we will choose the subset with the highest sample size.\n",
    "\n",
    "**Positive Tests:** This is our target variable. It was pulled from data.gov and contains weekly positive lab tests. We will shift the data one week backwards to create our target \"y\" and shift 1, 2, and 3 weeks forwards in order to create lagged case predictors. This database starts later than the others, so we selected 2010 as our starting year. The 2019 end date was chosen to avoid any effects from COVID-19 lockdowns.\n",
    "\n",
    "Important note: *Most studies estimate that only 5â€“20% of true infections are represented in testing, so our model will predict positive lab results for the next week (true cases) to avoid any assumptions on real flu levels. Our predictions could be adjusted to estimate actual cases, but we would need to study the relationship between actual flu cases and positive tests in the Chicago area, which we will not do.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dc559b",
   "metadata": {},
   "source": [
    "## Data Cleaning/Feature Engineering ##\n",
    "While SQL is not the most convenient method for this pipeline given the small datasets, it's used to demonstrate technical fluency as well as support scalability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ac32f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to find the dimension with the highest sample size, we can query the vaccination_rates table.\n",
    "pd.read_sql(\"\"\"\n",
    "SELECT DISTINCT Dimension\n",
    "FROM vaccination_rates\n",
    "WHERE \"Season/Survey Year\" BETWEEN '2010-11' AND '2019-2020'\n",
    "ORDER BY \"Sample Size\"\n",
    "\"\"\", conn);\n",
    "# The dimension with the highest sample size is >=6 months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "668c91a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x2517cfb39c0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create temporary views for cleaned data\n",
    "conn.execute(\"\"\"\n",
    "CREATE TEMP VIEW weather_clean AS\n",
    "SELECT\n",
    "    substr(date, 1, 10) AS date_clean,\n",
    "    tavg,\n",
    "    prcp\n",
    "FROM weather\n",
    "\"\"\")\n",
    "\n",
    "conn.execute(\"\"\"\n",
    "CREATE TEMP VIEW google_trends_clean AS\n",
    "SELECT\n",
    "    Month || '-01' AS month_date,\n",
    "    \"flu symptoms\",\n",
    "    fever,\n",
    "    \"Cough\",\n",
    "    \"sore throat\",\n",
    "    Tamiflu\n",
    "FROM google_trends\n",
    "\"\"\")\n",
    "\n",
    "conn.execute(\"\"\"\n",
    "CREATE TEMP VIEW vaccination_clean AS\n",
    "SELECT\n",
    "    \"Season/Survey Year\" AS flu_season,\n",
    "    Month,\n",
    "    CAST(\"Estimate (%)\" AS REAL) AS vax_rate\n",
    "FROM vaccination_rates\n",
    "WHERE Dimension = '>=6 Months'\n",
    "  AND \"Season/Survey Year\" BETWEEN '2010-11' AND '2019-20'\n",
    "\"\"\")\n",
    "\n",
    "conn.execute(\"\"\"\n",
    "CREATE TEMP VIEW positive_tests_clean AS\n",
    "SELECT\n",
    "    substr(week_start, 7, 4) || '-' || substr(week_start, 1, 2) || '-' || substr(week_start, 4, 2) AS week_start_clean,\n",
    "    count AS flu_cases\n",
    "FROM positive_tests\n",
    "WHERE characteristic_category = 'Influenza Positive'\n",
    "  AND characteristic_group = 'Total Positive'\n",
    "\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6128b39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query to join all cleaned data into a single DataFrame, using Left Joins to weather data, since it is the most granular.\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "    w.date_clean,\n",
    "    w.tavg,\n",
    "    w.prcp,\n",
    "\n",
    "    g.\"flu symptoms\",\n",
    "    g.fever,\n",
    "    g.\"Cough\",\n",
    "    g.\"sore throat\",\n",
    "    g.Tamiflu,\n",
    "\n",
    "    pt.flu_cases,\n",
    "\n",
    "    vc.vax_rate\n",
    "\n",
    "FROM weather_clean w\n",
    "\n",
    "LEFT JOIN google_trends_clean g\n",
    "    ON w.date_clean = g.month_date\n",
    "\n",
    "LEFT JOIN positive_tests_clean pt\n",
    "    ON w.date_clean = pt.week_start_clean\n",
    "\n",
    "LEFT JOIN vaccination_clean vc\n",
    "    ON\n",
    "        strftime('%Y', w.date_clean) || '-' || printf('%02d', CAST(strftime('%m', w.date_clean) AS INT)) = \n",
    "        (CASE \n",
    "            WHEN vc.Month IN (1, 2, 3) THEN CAST(CAST(substr(vc.flu_season, 1, 4) AS INT) + 1 AS TEXT) || '-' || printf('%02d', vc.Month)\n",
    "            ELSE substr(vc.flu_season, 1, 4) || '-' || printf('%02d', vc.Month)\n",
    "         END)\n",
    "\"\"\"\n",
    "df = pd.read_sql(query, conn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "663db31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date column and sort\n",
    "df['date_clean'] = pd.to_datetime(df['date_clean'])\n",
    "df = df.sort_values('date_clean')\n",
    "df = df.set_index('date_clean')\n",
    "\n",
    "# Forward-fill vax_rate (monthly)\n",
    "df['vax_rate'] = df['vax_rate'].ffill()\n",
    "\n",
    "#Detect drops in vax_rate > 20 and fill values before Oct 1 with last large value (to align reset with seasons)\n",
    "vax = df['vax_rate']\n",
    "drops = vax.diff() < -20\n",
    "drop_indices = df.index[drops]\n",
    "prev_idx = df.index[0]\n",
    "for drop_idx in drop_indices:\n",
    "    # Find last large value before drop\n",
    "    last_large = vax.loc[:drop_idx].iloc[-2]\n",
    "    # Find the season year for this drop\n",
    "    year = drop_idx.year if drop_idx.month >= 10 else drop_idx.year - 1\n",
    "    oct1 = pd.Timestamp(f'{year}-10-01')\n",
    "    # Fill values from previous drop (or start) up to Oct 1 with last_large\n",
    "    mask = (df.index >= prev_idx) & (df.index < oct1)\n",
    "    df.loc[mask, 'vax_rate'] = last_large\n",
    "    prev_idx = drop_idx\n",
    "\n",
    "# Convert vax_rate to fraction\n",
    "df['vax_rate'] = df['vax_rate'] / 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "385e4268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable: flu_cases shifted by 7 days in the future.\n",
    "df['y'] = df['flu_cases'].shift(-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e73b3b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lag features for flu_cases\n",
    "for lag in [7, 14, 21]:\n",
    "    df[f'flu_cases_lag_{lag}'] = df['flu_cases'].shift(lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "925fc34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create seasonality features\n",
    "df['in_season'] = ((df.index.month >= 10) | (df.index.month <= 4)).astype(int)\n",
    "df['season_year'] = np.where(df.index.month >= 10, df.index.year, df.index.year - 1)\n",
    "season_start = pd.to_datetime(df['season_year'].astype(str) + '-10-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ce3c6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work with weekly Google Trends data,\n",
    "trend_cols = ['Tamiflu', 'fever', 'Cough', 'flu symptoms', 'sore throat']\n",
    "\n",
    "for col in trend_cols:\n",
    "    df[col] = df[col].replace('<1', 0.5)\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "df[trend_cols] = df[trend_cols].ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0686dde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust trend columns for increase in Google usage over seasons (historic U.S. search volume data shows a linear 100% increase from 2010-2019)\n",
    "min_season = df['season_year'].min()\n",
    "max_season = df['season_year'].max()\n",
    "for season in df['season_year'].unique():\n",
    "    # Linear scaling: 1 for min_season, 0.5 for max_season\n",
    "    scale = 1 - 0.5 * (season - min_season) / (max_season - min_season)\n",
    "    df.loc[df['season_year'] == season, trend_cols] = df.loc[df['season_year'] == season, trend_cols] * scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ef727b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2013-01-28'], dtype='datetime64[ns]', name='date_clean', freq=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['tavg'].isna()].index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b880076e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the few missing temperature values with forward fill, and pcrp with 0\n",
    "df['tavg'] = df['tavg'].ffill()\n",
    "df['prcp'] = df['prcp'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8a24fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate weather slopes only for rows where y is not null, before dropping rows with missing y\n",
    "def weighted_slope_masked(series, mask, window=7):\n",
    "    import numpy as np\n",
    "    slopes = np.full(len(series), np.nan)\n",
    "    weights = np.arange(1, window+1)\n",
    "    valid_idx = np.where(mask)[0]\n",
    "    for idx in valid_idx:\n",
    "        if idx < window:\n",
    "            continue\n",
    "        y = series.iloc[idx-window:idx].values\n",
    "        if np.any(np.isnan(y)):\n",
    "            continue\n",
    "        x = np.arange(window)\n",
    "        w = weights\n",
    "        x_mean = np.average(x, weights=w)\n",
    "        y_mean = np.average(y, weights=w)\n",
    "        numerator = np.sum(w * (x - x_mean) * (y - y_mean))\n",
    "        denominator = np.sum(w * (x - x_mean)**2)\n",
    "        slopes[idx] = numerator / denominator if denominator != 0 else 0\n",
    "    return slopes\n",
    "\n",
    "mask_y = ~df['y'].isna()\n",
    "df['tavg_slope'] = weighted_slope_masked(df['tavg'], mask_y)\n",
    "# For prcp, use sum over previous 7 days (including only if y is not null)\n",
    "def sum_last_week(series, mask, window=7):\n",
    "    import numpy as np\n",
    "    sums = np.full(len(series), np.nan)\n",
    "    valid_idx = np.where(mask)[0]\n",
    "    for idx in valid_idx:\n",
    "        if idx < window:\n",
    "            continue\n",
    "        y = series.iloc[idx-window:idx].values\n",
    "        if np.any(np.isnan(y)):\n",
    "            continue\n",
    "        sums[idx] = np.sum(y)\n",
    "    return sums\n",
    "\n",
    "df['prcp_sum'] = sum_last_week(df['prcp'], mask_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c0312ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['flu_cases', 'prcp'], axis=1)\n",
    "df = df.dropna(subset=['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d08695a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values in lagged columns with 0s\n",
    "lagged_cols = ['flu_cases_lag_7', 'flu_cases_lag_14', 'flu_cases_lag_21']\n",
    "df[lagged_cols] = df[lagged_cols].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a26183b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of             tavg  flu symptoms      fever      Cough  sore throat  Tamiflu  \\\n",
       "date_clean                                                                   \n",
       "2010-09-26  12.3      5.000000  46.000000  27.000000    14.000000     0.00   \n",
       "2010-10-03   8.8      6.611111  41.555556  26.444444    12.277778     0.00   \n",
       "2010-10-10  20.6      6.611111  41.555556  26.444444    12.277778     0.00   \n",
       "2010-10-17  13.1      6.611111  41.555556  26.444444    12.277778     0.00   \n",
       "2010-10-24  17.5      6.611111  41.555556  26.444444    12.277778     0.00   \n",
       "...          ...           ...        ...        ...          ...      ...   \n",
       "2019-08-25  20.3      2.000000  28.000000  19.500000    12.500000     0.25   \n",
       "2019-09-01  19.4      3.000000  29.000000  25.500000    14.000000     0.50   \n",
       "2019-09-08  18.4      3.000000  29.000000  25.500000    14.000000     0.50   \n",
       "2019-09-15  21.4      3.000000  29.000000  25.500000    14.000000     0.50   \n",
       "2019-09-22  22.3      3.000000  29.000000  25.500000    14.000000     0.50   \n",
       "\n",
       "            vax_rate    y  flu_cases_lag_7  flu_cases_lag_14  \\\n",
       "date_clean                                                     \n",
       "2010-09-26     0.373  0.0              0.0               0.0   \n",
       "2010-10-03     0.225  0.0              0.0               0.0   \n",
       "2010-10-10     0.225  0.0              0.0               0.0   \n",
       "2010-10-17     0.225  0.0              0.0               0.0   \n",
       "2010-10-24     0.225  1.0              0.0               0.0   \n",
       "...              ...  ...              ...               ...   \n",
       "2019-08-25     0.024  1.0              1.0               1.0   \n",
       "2019-09-01     0.118  4.0              2.0               1.0   \n",
       "2019-09-08     0.118  0.0              1.0               2.0   \n",
       "2019-09-15     0.118  2.0              4.0               1.0   \n",
       "2019-09-22     0.118  5.0              0.0               4.0   \n",
       "\n",
       "            flu_cases_lag_21  in_season  season_year  tavg_slope  prcp_sum  \n",
       "date_clean                                                                  \n",
       "2010-09-26               0.0          0         2009   -1.340476       0.0  \n",
       "2010-10-03               0.0          1         2010   -0.540476       0.0  \n",
       "2010-10-10               0.0          1         2010    1.957143       0.0  \n",
       "2010-10-17               0.0          1         2010   -1.152381       2.5  \n",
       "2010-10-24               0.0          1         2010    0.535714       0.5  \n",
       "...                      ...        ...          ...         ...       ...  \n",
       "2019-08-25               3.0          0         2018   -0.961905      31.2  \n",
       "2019-09-01               1.0          0         2018   -0.283333      38.4  \n",
       "2019-09-08               1.0          0         2018   -0.390476      14.2  \n",
       "2019-09-15               2.0          0         2018   -0.526190      49.0  \n",
       "2019-09-22               1.0          0         2018    0.571429      15.5  \n",
       "\n",
       "[470 rows x 15 columns]>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ea06b84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facf23ef",
   "metadata": {},
   "source": [
    "## Exporting Cleaned Data ##\n",
    "All datesets were joined correctly, with the matching data index. Features were created, and NaN values were removed. The final cleaned dataset is now ready for modeling and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd1890c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"clean_flu_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b7814b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "math9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
